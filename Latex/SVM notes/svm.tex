\documentclass[twoside,12pt]{article}
\usepackage{amsmath,amsfonts,amsthm,fullpage,amssymb}
\usepackage{algorithm,hyperref}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{hyperref}

\begin{document}
\title{Notes on SVM}
\author {Shasha Liao \\ Georgia Tech}
\maketitle
\section{Classification}
\begin{itemize}
\item Decision boundary: $w^Tx + b = 0$
\item Positive class: (x, y) with $w^Tx + b \geq c$ and $y > 1$.
\item Negative class:  (x, y) with $w^Tx + b \leq -c$ and $y > 1$.
\item Margin: $\gamma = \frac{2c}{\|w\|}$, the distance between two hyperplanes $w^Tx + b = c$ and $w^Tx + b = -c$.
\item Correct classification condition: $( w^Tx_i + b )y_i \geq c$ for all data points $(x_i, y_i)$, $i = 1, 2, ..., m.$
\end{itemize}

\section{Maximum margin classifier}
Find decision boundary, i.e. $w$ and $b$ to maximize the margin. 

\begin{align}\label{eq1}
\max_{w,b} \gamma = \frac{2c}{\|w\|} \text{ s.t. } ( w^Tx_i + b )y_i \geq c \text{ for all } i.
\end{align}
This becomes an optimization problem with constraints. And the solution is invariant under the scaling of $c$. So we set $c = 1$ and the problem \eqref{eq1} is equivalent to the following constrained convex quadratic programming problem:
\begin{align}\label{eq2}
\min_{w,b} \|w\| \text{ s.t. } ( w^Tx_i + b )y_i \geq 1 \text{ for all } i = 1, 2, ..., m.
\end{align}
\begin{itemize}
\item Only a few of the constrains are relevant $\rightarrow$ \textbf{support vectors}
\item Kernel methods are introduced for nonlinear classification with nonlinear decision boundary.
\end{itemize}

To solve the optimization problem \eqref{eq2}, we need to use Lagrangian multipliers. 
\subsection{Lagrangian Duality}
The Lagrangian function:
\begin{align}\label{eq3}
L(w,b,\alpha) = \frac 1 2 w^Tw + \sum_{i=1}^m \alpha_i(1-y^i(w^Tx^i + b)).
\end{align}
The KKT conditions:
$$\frac{\partial L}{\partial w} = 0, \quad \frac{\partial L}{\partial b} = 0, \quad \alpha_i(1-y^i(w^Tx^i + b)) = 0, \alpha_i \geq 0,  \forall i.$$
The last equation implies that all the points that are not on the margin will have the corresponding $\alpha_i = 0$. The first equation gives $$w = \sum_{i=1}^m \alpha_i y^i x^i,$$ this tells us that $w$ is a linear combination of the support vectors, i.e., the vectors on the margins. 

From the KKT conditions we can solve for optimal $w^*$ and $b^*$, plug in them into $L$ we get the dual Lagrangian problem:
\begin{align}\label{eq4}
\max_{\alpha} L(w^*, b^*, \alpha) = \sum_{i}^m \alpha_i - \frac 1 2 \sum_{i,j}^m \alpha_i \alpha_j y^i y^j ((x^i)^Tx^j)\\
\text{ s.t. } \alpha_i \geq 0, i = 1, 2, ..., m\\
\sum_i^m \alpha_i y^i = 0. 
\end{align}
This is a constrained quadratic programming. $L(w^*, b^*, \alpha)$ is nice and convex, and the global maximum can be found. For more details, see \href{https://github.com/sliao7/CSE6740_Computational_Data_Analysis/blob/main/slides/svm.pdf}{the slides}. The optimal $\alpha_i$ can be found by solving the dual Lagrangian problem. \\

After that, we can find $w, b$ in terms of $\alpha_i$ for $i$ with $x_i$ on the margins, i.e., $x_i$ being support vector. \\

For a new test point $z$, compute $$w^Tz + b = \sum_{i \in \text{support vectors}} \alpha_i y^i((x^i)^Tz) + b.$$
If $w^Tz + b > 0$, $z$ is predicted to be in the positive class. Otherwise, $z$ is predicted to be in the negative class.

\section{Generalized SVM}
\begin{itemize}
\item Kernelized SVM 

Since the Lagrangian and the decision rule only depends on the pairwise inner products of the points, we can replace all the inner products with nonlinear kernel functions to obtain nonlinear decision boundary.
\item Soft-margin SVM 

When the data is not linearly separable, we allow a few points to violate the hard margin constraint and solve the following optimization problem:

\begin{align*}
\min_{w,b,\xi} \frac 1 2 \|w\|^2 + C\sum_{i=1}^m \xi^i \\
\text{ s.t. }  y^i(w^Tx^i + b) \geq 1 - \xi^i, \xi^i \geq 0, \forall i.
\end{align*}
Question: can $\xi^i \geq 1$?

\end{itemize}
\end{document}