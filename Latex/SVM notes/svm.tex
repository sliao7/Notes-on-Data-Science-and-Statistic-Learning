\documentclass[twoside,12pt]{article}
\usepackage{amsmath,amsfonts,amsthm,fullpage,amssymb}
\usepackage{algorithm,hyperref}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{hyperref}

\begin{document}
\title{Notes on SVM}
\author {Shasha Liao \\ Georgia Tech}
\maketitle
\section{Classification}
\begin{itemize}
\item Decision boundary: $w^Tx + b = 0$
\item Positive class: (x, y) with $w^Tx + b \geq c$ and $y > 1$.
\item Negative class:  (x, y) with $w^Tx + b \leq -c$ and $y > 1$.
\item Margin: $\gamma = \frac{2c}{\|w\|}$, the distance between two hyperplanes $w^Tx + b = c$ and $w^Tx + b = -c$.
\item Correct classification condition: $( w^Tx_i + b )y_i \geq c$ for all data points $(x_i, y_i)$, $i = 1, 2, ..., m.$
\end{itemize}

\section{Maximum margin classifier}
Find decision boundary, i.e. $w$ and $b$ to maximize the margin. 

\begin{align}\label{eq1}
\max_{w,b} \gamma = \frac{2c}{\|w\|} \text{ s.t. } ( w^Tx_i + b )y_i \geq c \text{ for all } i.
\end{align}
This becomes an optimization problem with constraints. And the solution is invariant under the scaling of $c$. So we set $c = 1$ and the problem \eqref{eq1} is equivalent to the following constrained convex quadratic programming problem:
\begin{align}\label{eq2}
\min_{w,b} \|w\| \text{ s.t. } ( w^Tx_i + b )y_i \geq 1 \text{ for all } i = 1, 2, ..., m.
\end{align}
\begin{itemize}
\item Only a few of the constrains are relevant $\rightarrow$ \textbf{support vectors}
\item Kernel methods are introduced for nonlinear classification with nonlinear decision boundary.
\end{itemize}

To solve the optimization problem \eqref{eq2}, we need to use Lagrangian multipliers. 
\subsection{Lagrangian Duality}
The Lagrangian function:
\begin{align}\label{eq3}
L(w,b,\alpha) = \frac 1 2 w^Tw + \sum_{i=1}^m \alpha_i(1-y^i(w^Tx^i + b)).
\end{align}
The KKT conditions:
$$\frac{\partial L}{\partial w} = 0, \quad \frac{\partial L}{\partial b} = 0, \quad \alpha_i(1-y^i(w^Tx^i + b)) = 0, \alpha_i \geq 0,  \forall i.$$
The last equation implies that all the points that are not on the margin will have the corresponding $\alpha_i = 0$. The first equation gives $$w = \sum_{i=1}^m \alpha_i y^i x^i,$$ this tells us that $w$ is a linear combination of the support vectors, i.e., the vectors on the margins. 

From the KKT conditions we can solve for optimal $w^*$ and $b^*$, plug in them into $L$ we get the dual Lagrangian problem:
\begin{align}\label{eq4}
\max_{\alpha} L(w^*, b^*, \alpha) = \sum_{i}^m \alpha_i - \frac 1 2 \sum_{i,j}^m \alpha_i \alpha_j y^i y^j ((x^i)^Tx^j)\\
\text{ s.t. } \alpha_i \geq 0, i = 1, 2, ..., m\\
\sum_i^m \alpha_i y^i = 0. 
\end{align}
This is a constrained quadratic programming. $L(w^*, b^*, \alpha)$ is nice and convex, and the global maximum can be found. For more details, see \href{https://github.com/sliao7/CSE6740_Computational_Data_Analysis/blob/main/slides/svm.pdf}{the slides}. The optimal $\alpha_i$ can be found by solving the dual Lagrangian problem. \\

After that, we can find $w, b$ in terms of $\alpha_i$ for $i$ with $x_i$ on the margins, i.e., $x_i$ being support vector. \\

For a new test point $z$, compute $$w^Tz + b = \sum_{i \in \text{support vectors}} \alpha_i y^i((x^i)^Tz) + b.$$
If $w^Tz + b > 0$, $z$ is predicted to be in the positive class. Otherwise, $z$ is predicted to be in the negative class.

\section{Generalized SVM}
\begin{itemize}
\item Kernelized SVM 

Since the Lagrangian and the decision rule only depends on the pairwise inner products of the points, we can replace all the inner products with nonlinear kernel functions to obtain nonlinear decision boundary.
\item Soft-margin SVM 

When the data is not linearly separable, we allow a few points to violate the hard margin constraint and solve the following optimization problem:

\begin{align*}
\min_{w,b,\xi} \frac 1 2 \|w\|^2 + C\sum_{i=1}^m \xi^i \\
\text{ s.t. }  y^i(w^Tx^i + b) \geq 1 - \xi^i, \xi^i \geq 0, \forall i.
\end{align*}
Now the examples are allowed to have functional margin less than 1, and if an example has a margin $ 1 - \xi^i$, we would pay a cost of the objective function being increased by $C\xi^i$. The parameter $C$ controls the relative weighting between the twin goals of making the $\|w\|^2$ large (which makes the margin small) and of ensuring that most examples have functional margin at least 1. \\
\begin{itemize}
\item If $C$ is large, the model will not allow examples to have functional margin less than 1. Only in this case, we achieve maximal margin classifier. 
\item If $C$ is small, the model allows for incorrectly classified examples and has a wider margin. This is good for the situation when we have outliers and we don't want to affect the decision boundary. 
\end{itemize}
\section{Choosing SVM Parameters}
Choosing $C$:
\begin{itemize}
\item Large $C \Rightarrow$ overfitting, higher variance,  lower bias
\item Small $C \Rightarrow$ underfitting, low variance, high bias 
\end{itemize}
Choose $\sigma^2$ for Gaussian Kernel:
\begin{itemize}
\item Large $\sigma^2 \Rightarrow$ underfitting, low variance, high bias 
\item Small $\sigma^2 \Rightarrow$ overfitting, higher variance,  lower bias
\end{itemize}

The above optimization problem can be solved using SMO algorithm. 
\end{itemize}
\end{document}