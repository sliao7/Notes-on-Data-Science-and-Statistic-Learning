\documentclass[twoside,12pt]{article}
\usepackage{amsmath,amsfonts,amsthm,fullpage,amssymb}
\usepackage{algorithm,hyperref}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{hyperref}

\begin{document}
\title{Notes on Boosting}
\author {Shasha Liao \\ Georgia Tech}
\maketitle
\textbf{Rationale: Combination of methods} \\
General machine learning tasks, there is no algorithm that is always the most accurate. \\
"Can a set of weak learners create a single strong learner? " - Kearns and Valiant (1988, 1989) \\

\section{Two main approaches}
\begin{itemize}
\item Boosting \\
- Run weak learner on weighted example set \\
- Combine weak learners linearly \\
- Require knowledge on the performance of weak learner 
\item Bagging \\
- Run weak learners on bootstrap replicates of the training set \\
- Average weak learners \\
- Reduces variance
\end{itemize}

\section{Boosting}
Boosting: general methods of converting weak learners into a more powerful one
\begin{itemize}
\item Each learner is independent on the previous one and focuses on the previous one's errors
\item Data that are incorrectly predicted in the previous rounds are weighted more heavily when deciding a new learner.
\end{itemize}

Questions: 
\begin{itemize}
\item How to adjust weights for data?
\item How to combine learners? 
\end{itemize}
Boosting Setup
\begin{itemize}
\item Given a set of base classifiers $\{h_1, h_2, ...\}$, $h_i : X \rightarrow \{1, -1\}$.
\item Training data: $$(x^i, y^i), i = 1, ..., m, x^i \in X \text{ and } y^i \in \{1, -1\}$$
\item Construct: \\
- a sequence of distributions (weights on data sum up to 1) $D(i), i = 1, ..., m.$ \\
- a sequence $\{\alpha_k\}$ of nonnegative weights of base classifiers \\
- final classifier performs significantly better than any base classifier \\
\end{itemize}

AdaBoost: 
\begin{itemize}
\item Adaptive in the sense that subsequent weak learners are adjusted in favor of those instances misclassified by previous classifiers
\item Distribution and weights are adjusted adaptively. 
\end{itemize}
There are many variant of AdaBoost. Here is a basic one:
\begin{enumerate}
\item Construct $D_t: t = 1, ..., T$, where $T$ is the number of iterations for updating distribution and weights. 
\item Initialize $D_1(i)$ for $i = 1, 2, ..., m.$
\item Given $D_t$ and weak learner $h_t$: \\
Compute weighted misclassification rate: $$\epsilon_t = \sum_{i=1}^m D_t(i) \mathbb{I}\{ y^i \neq h_t(x^i) \} $$ 
Compute weight for $h_t$: $$\alpha_t = \frac 1 2 \ln( \frac{1 - \epsilon_t}{\epsilon_t} )$$

\item Update weights for data: \\
$$D_{t+1}(i) = \frac{D_t(i)}{Z_t} e^{-\alpha_t y^i h_t(x^i)} = \frac{D_t(i)}{Z_t} \times  \left\{ \begin{array}{lcr} e^{-\alpha_t} & \mbox{ if }  & y^i = h_t(x^i) \\ e^{\alpha_t} &\mbox{ otherwise. }&\end{array}\right. $$
Here $Z_t$ = normalizing constant. $Z_t = \sum_{i=1}^m D_t(i) e^{-\alpha_t y^i h_t(x^i)}$. 

\item Final classifier: $$H(x) = \text{sign}(\sum_{t=1}^T \alpha_t h_t(x)).$$
\end{enumerate}
Note that large $\epsilon_t$ gives small weight $\alpha_t$ and when $\epsilon_t > 0.5$, $\alpha_t < 0$ and the points classified correctly by $h_t$ will have a higher weight in the next round. When $\epsilon_t < 0.5$, $\alpha_t > 0$ and the points classified incorrectly by $h_t$ will have a higher weight in the next round. Check a toy example in the  \href{https://github.com/sliao7/CSE6740\_Computational\_Data\_Analysis/blob/main/slides/boosting.pdf}{slides} for better understanding. \\


\noindent Question: How are the weak learners $h_t$ chosen? How are the updating rules in step 3 and 4 decided?
\begin{itemize}
\item Combined classifier: $$f_t(x) = \alpha_1 h_1(x; \theta_1) + \cdots +  \alpha_t h_t(x; \theta_t)$$
\item Exponential Loss: $$\hat{L}(f_t) = \frac 1 m \sum_{i = 1}^m \exp(-y^if_t(x^i)) =\frac 1 m \Pi_{s=1}^t Z_s \sum_{i=1}^m D_t(i) e^{-\alpha_t y^i h_t(x^i;\theta_t)}. $$
\item We find $h_t$ by finding $\theta_t$ that minimizes $\hat{L}(f_t)$, which also approximately minimizes $\epsilon_t$. 
\item We find $\alpha_t$ by setting $$\frac{\partial \hat{L}}{\partial \alpha_t} = 0,$$ which gives the updating rule in step 3.
\item Why do we update $D_t$ the way in step 4?
\end{itemize}
See more detailed steps in the  \href{https://github.com/sliao7/CSE6740\_Computational\_Data\_Analysis/blob/main/slides/boosting.pdf}{slides}.

\section{Extensions}
\begin{itemize}
\item There are many other more recent algorithms such as: \\
\textbf{LPBoost, TotalBoost, BrownBoost, XGBoost, MadaBoost, LogitBoost}
\item Generalization: \textbf{Gradient boosting} is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically \textbf{decision trees}; allowing optimization of an arbitrary differentiable loss function. 
\end{itemize}


\end{document}